# CI2024_project-work
The main idea of the project is to provide an equation fi for each given dataset (xi,yi) to minimize the difference between yi and fi(xi) calculated with MSE. The idea is to used genetic programming as used in the course by creating trees of functions and using genetic algorithm to found the optimal tree describing the optimal function.

The first thing I needed to do was to create the problem using a class. I want to create a structure that describe a function in form of a tree to be easy to manipulate, mutate and combine using genetic algorithm. One node will be define using a class by a mathematic operation and his two children, we will also associate it a score (result of operation of the two children). A leaf will be a special type of node with no mathematic operation nor children, it can be differentiated from the other with his value self.leaf=True. A leaf will only have a value witch can be either one array of x or  float number between -50 and 50. A tree will simply be a node with other children as nodes. When creating a node I can choose to create a particular node with a specific value, specific children and specific mathematic operations or ask the function to create it randomly by only saying if I want a leaf or not. To have a quick understanding of what is contained in a tree I associated a value text which express the function corresponding to the tree in an readable way. The function of the Node class called print_text directly print the text of the tree.

When a node is not a leaf it can either have one or two children. I created two list of possible actions, one to use with one children (such as log, exp, cos…) and one with two children (add, multiplate,divide,…). The functions used are:
np_functions = [np.add, np.subtract, np.multiply, np.divide,np.maximum,np.minimum, np.power]
np_functions_1D=[safe_log, safe_log10, np.exp, np.sin, np.cos, np.tan, safe_sqrt,np.sign]

For some functions I created safe versions to match the domain of definition. For instance for logarithm functions I take the maximum argument of the input and add 10*-9 to respect the R+ * definition domain. 
The tree must be easily to modify so we could create neighbours. So I created  function update that update the value and the text of the node. When I modify a child of a node I can just call this function on the node and all parameters will match the modifications. They also must be easy to evaluate, I implemented MSE, MAE and R² functions to evaluate the result of a tree considering a given y.
To first evaluate my structure I created a function that create a tree in a random way by entering the value of x and the maximum depth of a tree needed. Controlling depth of trees is very important in this project to avoid ending up with really large and long to calculate equations. I implemented a really simple function select best tree which create a great number of random trees and return the best one considering the MSE between the result of the tree and y.

## Simulated Annealing:

I started by implementing Simulated Annealing which is an algorithm I already knew well and implemented in lab. I created a class Tree_problem that define the problem with the value of x and y and a random first tree. I adapted my simulated Annealing algorithm to this problem. The compilation was fast and I was able to have a better understanding of the weaknesses in my structure. For example this is when I decided to labels my leafs instead of checking the children for each node. I was also able to try diverse mutations function. In fact I have various mutation function. Switch child exchange the two children (x[0]/2 become 2/x[0]) but I did not keep it as it did not seem to improve the model. Change action replace an action by another, cut_branch transform a branch (node with children) into a leaf while expand_branch take a leaf and transform it in a branch of depth one (it will have one or two children with one of them being the value of the original leaf). Finally, modify_leaf_number (that I added after a trying and seeing the results with the genetic algorithm) will look for a leaf containing a number and not an array of x and will multiply or divide it. By doing so, the algorithm can fine-tune specific parameters dynamically, leading to potentially better optimization results. I also added afterward the encapsulation mutation which uses a 1D-function over an original tree to create a mutation.

To combine this mutation a function create neighbour randomly chooses a mutation to perform while taking in consideration the depth of the tree. It will grow the tree if he is already to big. The selected function is applied to a random suitable node of the tree. Plus all superior branches are updated on any modification.

## Genetic algorithm:

I implemented a genetic algorithm where we can control the size of the population, the number maximum of iterations, the algorithm of selection and the importance of elitism (for the elitism rank or elitism roulette).  First I created a function that allow the creation of a random population given a size of population and a problem. Once the original population is created we select the best tree (depending on the MSE between tree result and y) and keep it in memory as our best tree and the calculated MSE is added to the fit_list and save as the current best MSE. Then for each iteration we have to create a new population set coming from the previous one. For each child we select two parents randomly from a set of pre-selected parents. 
I tried different selection function to obtain this set: elitism selection, roulette selection, rank selection and combination between elitism and roulette and elitism and rank. The last one seemed to be more efficient to allow new assets to come through while keeping our best parents assets. 
Once the parents are selected I generate a child node by putting two random branch of the parents as children in the child node (with a random action). For this I use the class function random branch which return a branch of each parents (it can be the all tree or just a part). For the mutations, I use the random mutation selection explain before.
In my algorithm, if no improvement is observed after max iteration I replace a significant portion of the population (currently set at 90%). This mechanism goal is to escape local minima while encouraging the exploration of new combinations.

## MSE flexible:

When I ran my program, I observed that it performed efficiently on certain problems but struggled significantly with others. Upon analysing the output, I realized that the algorithm's reliance on the Mean Squared Error (MSE) heavily penalized points far from the target. This resulted in favoring solutions with sizes and centers closer to y, while failing to accurately capture the distribution of y.
My idea was to create a flexible MSE which will multiply the result by an integer in order to have approximately the same size as y and add a fixed number to each row in order to have the same middle as y.  Last year at my home university I studied the mathematics behind machine learning (I learned about the mathematical implication of SVM or linear regression). The linear regression process uses a scaling alpha based on the variance of the result and the covariance between the result and y. And a beta which take into consideration the mean of both the result and y to recentre the result. It makes a lot of sense to use it here as we want to obtain a linear relation between the result and y (y=result would be perfect).
If we take a look at this equation:
 
By applying the MSE function on the linear component and y we will only have to care about how big the error epsilon is which will favorise equations with similar distributions as y. Plus it is easy to implement and fast to compute.
Alpha=Cov(result,y)/Var(result) and Beta=mean(y)-mean(result)
